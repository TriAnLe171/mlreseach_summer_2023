{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt    # for plotting the images\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "#!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "\n",
    "#!tar xvf VOCtrainval_06-Nov-2007.tar\n",
    "#!tar xvf VOCtest_06-Nov-2007.tar\n",
    "\n",
    "#!rm VOCtrainval_06-Nov-2007.tar\n",
    "#!rm VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Build Annotations.')\n",
    "parser.add_argument('dir', default='..', help='Annotations.')\n",
    "\n",
    "sets = [('2007', 'train'), ('2007', 'val'), ('2007', 'test')]\n",
    "\n",
    "classes_num = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5,\n",
    "               'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11,\n",
    "               'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16,\n",
    "               'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
    "\n",
    "\n",
    "def convert_annotation(year, image_id, f):\n",
    "    in_file = os.path.join('VOCdevkit/VOC%s/Annotations/%s.xml' % (year, image_id))\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        classes = list(classes_num.keys())\n",
    "        if cls not in classes or int(difficult) == 1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text),\n",
    "             int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text))\n",
    "        f.write(' ' + ','.join([str(a) for a in b]) + ',' + str(cls_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007 train\n",
      "2007 val\n",
      "2007 test\n"
     ]
    }
   ],
   "source": [
    "for year, image_set in sets:\n",
    "  print(year, image_set)\n",
    "  with open(os.path.join('VOCdevkit/VOC%s/ImageSets/Main/%s.txt' % (year, image_set)), 'r') as f:\n",
    "      image_ids = f.read().strip().split()\n",
    "  with open(os.path.join(\"VOCdevkit\", '%s_%s.txt' % (year, image_set)), 'w') as f:\n",
    "      for image_id in image_ids:\n",
    "          f.write('%s/VOC%s/JPEGImages/%s.jpg' % (\"VOCdevkit\", year, image_id))\n",
    "          convert_annotation(year, image_id, f)\n",
    "          f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def read(image_path, label):\n",
    "    image = cv.imread(image_path)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image_h, image_w = image.shape[0:2]\n",
    "    image = cv.resize(image, (448, 448))\n",
    "    image = image / 255.\n",
    "\n",
    "    label_matrix = np.zeros([7, 7, 30])\n",
    "    for l in label:\n",
    "        l = l.split(',')\n",
    "        l = np.array(l, dtype=np.int32)\n",
    "        xmin = l[0]\n",
    "        ymin = l[1]\n",
    "        xmax = l[2]\n",
    "        ymax = l[3]\n",
    "        cls = l[4]\n",
    "        x = (xmin + xmax) / 2 / image_w\n",
    "        y = (ymin + ymax) / 2 / image_h\n",
    "        w = (xmax - xmin) / image_w\n",
    "        h = (ymax - ymin) / image_h\n",
    "        loc = [7 * x, 7 * y]\n",
    "        loc_i = int(loc[1])\n",
    "        loc_j = int(loc[0])\n",
    "        y = loc[1] - loc_i\n",
    "        x = loc[0] - loc_j\n",
    "\n",
    "        if label_matrix[loc_i, loc_j, 24] == 0:\n",
    "            label_matrix[loc_i, loc_j, cls] = 1\n",
    "            label_matrix[loc_i, loc_j, 20:24] = [x, y, w, h]\n",
    "            label_matrix[loc_i, loc_j, 24] = 1  # response\n",
    "\n",
    "    return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, images, labels, batch_size) :\n",
    "    self.images = images\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int32)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "    train_image = []\n",
    "    train_label = []\n",
    "\n",
    "    for i in range(0, len(batch_x)):\n",
    "      img_path = batch_x[i]\n",
    "      label = batch_y[i]\n",
    "      image, label_matrix = read(img_path, label)\n",
    "      train_image.append(image)\n",
    "      train_label.append(label_matrix)\n",
    "    return np.array(train_image), np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "val_datasets = []\n",
    "\n",
    "with open(os.path.join(\"VOCdevkit\", '2007_train.txt'), 'r') as f:\n",
    "    train_datasets = train_datasets + f.readlines()\n",
    "with open(os.path.join(\"VOCdevkit\", '2007_val.txt'), 'r') as f:\n",
    "    val_datasets = val_datasets + f.readlines()\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "for item in train_datasets:\n",
    "  item = item.replace(\"\\n\", \"\").split(\" \")\n",
    "  X_train.append(item[0])\n",
    "  arr = []\n",
    "  for i in range(1, len(item)):\n",
    "    arr.append(item[i])\n",
    "  Y_train.append(arr)\n",
    "\n",
    "for item in val_datasets:\n",
    "  item = item.replace(\"\\n\", \"\").split(\" \")\n",
    "  X_val.append(item[0])\n",
    "  arr = []\n",
    "  for i in range(1, len(item)):\n",
    "    arr.append(item[i])\n",
    "  Y_val.append(arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 448, 448, 3)\n",
      "(4, 7, 7, 30)\n",
      "(4, 448, 448, 3)\n",
      "(4, 7, 7, 30)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "my_training_batch_generator = My_Custom_Generator(X_train, Y_train, batch_size)\n",
    "\n",
    "my_validation_batch_generator = My_Custom_Generator(X_val, Y_val, batch_size)\n",
    "\n",
    "x_train, y_train = my_training_batch_generator.__getitem__(0)\n",
    "x_val, y_val = my_training_batch_generator.__getitem__(0)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "class Yolo_Reshape(tf.keras.layers.Layer):\n",
    "  def __init__(self, target_shape):\n",
    "    super(Yolo_Reshape, self).__init__()\n",
    "    self.target_shape = tuple(target_shape)\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({\n",
    "        'target_shape': self.target_shape\n",
    "    })\n",
    "    return config\n",
    "\n",
    "  def call(self, input):\n",
    "    # grids 7x7\n",
    "    S = [self.target_shape[0], self.target_shape[1]]\n",
    "    # classes\n",
    "    C = 20\n",
    "    # no of bounding boxes per grid\n",
    "    B = 2\n",
    "\n",
    "    idx1 = S[0] * S[1] * C\n",
    "    idx2 = idx1 + S[0] * S[1] * B\n",
    "    \n",
    "    # class probabilities\n",
    "    class_probs = K.reshape(input[:, :idx1], (K.shape(input)[0],) + tuple([S[0], S[1], C]))\n",
    "    class_probs = K.softmax(class_probs)\n",
    "\n",
    "    #confidence\n",
    "    confs = K.reshape(input[:, idx1:idx2], (K.shape(input)[0],) + tuple([S[0], S[1], B]))\n",
    "    confs = K.sigmoid(confs)\n",
    "\n",
    "    # boxes\n",
    "    boxes = K.reshape(input[:, idx2:], (K.shape(input)[0],) + tuple([S[0], S[1], B * 4]))\n",
    "    boxes = K.sigmoid(boxes)\n",
    "\n",
    "    outputs = K.concatenate([class_probs, confs, boxes])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_48 (Conv2D)          (None, 448, 448, 64)      9472      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 224, 224, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 224, 224, 192)     110784    \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 112, 112, 192)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 112, 112, 128)     24704     \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 112, 112, 256)     295168    \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 112, 112, 256)     65792     \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 112, 112, 512)     1180160   \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 56, 56, 512)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 56, 56, 256)       131328    \n",
      "                                                                 \n",
      " conv2d_55 (Conv2D)          (None, 56, 56, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_56 (Conv2D)          (None, 56, 56, 256)       131328    \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 56, 56, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_58 (Conv2D)          (None, 56, 56, 256)       131328    \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 56, 56, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 56, 56, 256)       131328    \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 56, 56, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 56, 56, 512)       262656    \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          (None, 56, 56, 1024)      4719616   \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 28, 28, 1024)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          (None, 28, 28, 512)       524800    \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          (None, 28, 28, 1024)      4719616   \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 28, 28, 512)       524800    \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 28, 28, 1024)      4719616   \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 28, 28, 1024)      9438208   \n",
      "                                                                 \n",
      " conv2d_69 (Conv2D)          (None, 14, 14, 1024)      9438208   \n",
      "                                                                 \n",
      " conv2d_70 (Conv2D)          (None, 12, 12, 1024)      9438208   \n",
      "                                                                 \n",
      " conv2d_71 (Conv2D)          (None, 10, 10, 1024)      9438208   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 102400)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               52429312  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1470)              1506750   \n",
      "                                                                 \n",
      " yolo__reshape_2 (Yolo_Resha  (None, 7, 7, 30)         0         \n",
      " pe)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,617,342\n",
      "Trainable params: 114,617,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "lrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "\n",
    "nb_boxes=1\n",
    "grid_w=7\n",
    "grid_h=7\n",
    "cell_w=64\n",
    "cell_h=64\n",
    "img_w=grid_w*cell_w\n",
    "img_h=grid_h*cell_h\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size= (7, 7), strides=(1, 1), input_shape =(img_h, img_w, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=192, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=256, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=256, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=512, kernel_size= (1, 1), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), padding = 'same', activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), strides=(2, 2), padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "model.add(Conv2D(filters=1024, kernel_size= (3, 3), activation=lrelu, kernel_regularizer=l2(5e-4)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1470, activation='sigmoid'))\n",
    "model.add(Yolo_Reshape(target_shape=(7,7,30)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
    "\n",
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    (0, 0.01),\n",
    "    (75, 0.001),\n",
    "    (105, 0.0001),\n",
    "]\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def xywh2minmax(xy, wh):\n",
    "    xy_min = xy - wh / 2\n",
    "    xy_max = xy + wh / 2\n",
    "\n",
    "    return xy_min, xy_max\n",
    "\n",
    "\n",
    "def iou(pred_mins, pred_maxes, true_mins, true_maxes):\n",
    "    intersect_mins = K.maximum(pred_mins, true_mins)\n",
    "    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    pred_wh = pred_maxes - pred_mins\n",
    "    true_wh = true_maxes - true_mins\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores = intersect_areas / union_areas\n",
    "\n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "def yolo_head(feats):\n",
    "    # Dynamic implementation of conv dims for fully convolutional model.\n",
    "    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n",
    "    # In YOLO the height index is the inner most iteration.\n",
    "    conv_height_index = K.arange(0, stop=conv_dims[0])\n",
    "    conv_width_index = K.arange(0, stop=conv_dims[1])\n",
    "    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n",
    "\n",
    "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
    "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
    "    conv_width_index = K.tile(\n",
    "        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n",
    "    conv_width_index = K.flatten(K.transpose(conv_width_index))\n",
    "    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n",
    "    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n",
    "    conv_index = K.cast(conv_index, K.dtype(feats))\n",
    "\n",
    "    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n",
    "\n",
    "    box_xy = (feats[..., :2] + conv_index) / conv_dims * 448\n",
    "    box_wh = feats[..., 2:4] * 448\n",
    "\n",
    "    return box_xy, box_wh\n",
    "\n",
    "\n",
    "def yolo_loss(y_true, y_pred):\n",
    "    label_class = y_true[..., :20]  # ? * 7 * 7 * 20\n",
    "    label_box = y_true[..., 20:24]  # ? * 7 * 7 * 4\n",
    "    response_mask = y_true[..., 24]  # ? * 7 * 7\n",
    "    response_mask = K.expand_dims(response_mask)  # ? * 7 * 7 * 1\n",
    "\n",
    "    predict_class = y_pred[..., :20]  # ? * 7 * 7 * 20\n",
    "    predict_trust = y_pred[..., 20:22]  # ? * 7 * 7 * 2\n",
    "    predict_box = y_pred[..., 22:]  # ? * 7 * 7 * 8\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
    "    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n",
    "\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
    "    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n",
    "\n",
    "    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n",
    "    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2\n",
    "    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n",
    "\n",
    "    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n",
    "\n",
    "    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n",
    "    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n",
    "    confidence_loss = no_object_loss + object_loss\n",
    "    confidence_loss = K.sum(confidence_loss)\n",
    "\n",
    "    class_loss = response_mask * K.square(label_class - predict_class)\n",
    "    class_loss = K.sum(class_loss)\n",
    "\n",
    "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
    "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
    "\n",
    "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
    "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
    "\n",
    "    box_mask = K.expand_dims(box_mask)\n",
    "    response_mask = K.expand_dims(response_mask)\n",
    "\n",
    "    box_loss = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n",
    "    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n",
    "    box_loss = K.sum(box_loss)\n",
    "\n",
    "    loss = confidence_loss + class_loss + box_loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model.compile(loss=yolo_loss ,optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: Learning rate is 0.0100.\n",
      "Epoch 1/135\n",
      "625/625 [==============================] - 80s 120ms/step - loss: 66.9706 - val_loss: 60.6280\n",
      "\n",
      "Epoch 00001: Learning rate is 0.0100.\n",
      "Epoch 2/135\n",
      "625/625 [==============================] - 72s 114ms/step - loss: 76.5495 - val_loss: 86.1301\n",
      "\n",
      "Epoch 00002: Learning rate is 0.0100.\n",
      "Epoch 3/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 91.2656 - val_loss: 93.6416\n",
      "\n",
      "Epoch 00003: Learning rate is 0.0100.\n",
      "Epoch 4/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 96.8230 - val_loss: 98.0865\n",
      "\n",
      "Epoch 00004: Learning rate is 0.0100.\n",
      "Epoch 5/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 97.5200 - val_loss: 94.7495\n",
      "\n",
      "Epoch 00005: Learning rate is 0.0100.\n",
      "Epoch 6/135\n",
      "625/625 [==============================] - 72s 114ms/step - loss: 94.9233 - val_loss: 93.0677\n",
      "\n",
      "Epoch 00006: Learning rate is 0.0100.\n",
      "Epoch 7/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 94.0199 - val_loss: 93.7901\n",
      "\n",
      "Epoch 00007: Learning rate is 0.0100.\n",
      "Epoch 8/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 97.9933 - val_loss: 104.8329\n",
      "\n",
      "Epoch 00008: Learning rate is 0.0100.\n",
      "Epoch 9/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 104.9333 - val_loss: 110.5721\n",
      "\n",
      "Epoch 00009: Learning rate is 0.0100.\n",
      "Epoch 10/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 192.3754 - val_loss: 174.9103\n",
      "\n",
      "Epoch 00010: Learning rate is 0.0100.\n",
      "Epoch 11/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 143.2810 - val_loss: 127.7221\n",
      "\n",
      "Epoch 00011: Learning rate is 0.0100.\n",
      "Epoch 12/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 116.5932 - val_loss: 113.8668\n",
      "\n",
      "Epoch 00012: Learning rate is 0.0100.\n",
      "Epoch 13/135\n",
      "625/625 [==============================] - 70s 113ms/step - loss: 105.3652 - val_loss: 106.5564\n",
      "\n",
      "Epoch 00013: Learning rate is 0.0100.\n",
      "Epoch 14/135\n",
      "625/625 [==============================] - 71s 113ms/step - loss: 100.4761 - val_loss: 102.5439\n",
      "\n",
      "Epoch 00014: Learning rate is 0.0100.\n",
      "Epoch 15/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 97.6138 - val_loss: 100.5481\n",
      "\n",
      "Epoch 00015: Learning rate is 0.0100.\n",
      "Epoch 16/135\n",
      "625/625 [==============================] - 73s 116ms/step - loss: 95.8726 - val_loss: 99.5735\n",
      "\n",
      "Epoch 00016: Learning rate is 0.0100.\n",
      "Epoch 17/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 94.8759 - val_loss: 99.0622\n",
      "\n",
      "Epoch 00017: Learning rate is 0.0100.\n",
      "Epoch 18/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 94.0646 - val_loss: 98.1427\n",
      "\n",
      "Epoch 00018: Learning rate is 0.0100.\n",
      "Epoch 19/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 93.3729 - val_loss: 96.7782\n",
      "\n",
      "Epoch 00019: Learning rate is 0.0100.\n",
      "Epoch 20/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 93.0140 - val_loss: 96.5142\n",
      "\n",
      "Epoch 00020: Learning rate is 0.0100.\n",
      "Epoch 21/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 91.6767 - val_loss: 97.9264\n",
      "\n",
      "Epoch 00021: Learning rate is 0.0100.\n",
      "Epoch 22/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 92.3424 - val_loss: 94.9007\n",
      "\n",
      "Epoch 00022: Learning rate is 0.0100.\n",
      "Epoch 23/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 233.7707 - val_loss: 192.6731\n",
      "\n",
      "Epoch 00023: Learning rate is 0.0100.\n",
      "Epoch 24/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 175.0392 - val_loss: 162.3804\n",
      "\n",
      "Epoch 00024: Learning rate is 0.0100.\n",
      "Epoch 25/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 153.9322 - val_loss: 147.5806\n",
      "\n",
      "Epoch 00025: Learning rate is 0.0100.\n",
      "Epoch 26/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 142.3813 - val_loss: 138.7947\n",
      "\n",
      "Epoch 00026: Learning rate is 0.0100.\n",
      "Epoch 27/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 135.1561 - val_loss: 133.1642\n",
      "\n",
      "Epoch 00027: Learning rate is 0.0100.\n",
      "Epoch 28/135\n",
      "625/625 [==============================] - 72s 114ms/step - loss: 130.4504 - val_loss: 129.5098\n",
      "\n",
      "Epoch 00028: Learning rate is 0.0100.\n",
      "Epoch 29/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 127.3396 - val_loss: 127.0417\n",
      "\n",
      "Epoch 00029: Learning rate is 0.0100.\n",
      "Epoch 30/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 125.1707 - val_loss: 125.3367\n",
      "\n",
      "Epoch 00030: Learning rate is 0.0100.\n",
      "Epoch 31/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 123.6992 - val_loss: 124.1146\n",
      "\n",
      "Epoch 00031: Learning rate is 0.0100.\n",
      "Epoch 32/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 122.5147 - val_loss: 123.2240\n",
      "\n",
      "Epoch 00032: Learning rate is 0.0100.\n",
      "Epoch 33/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 121.7569 - val_loss: 122.5346\n",
      "\n",
      "Epoch 00033: Learning rate is 0.0100.\n",
      "Epoch 34/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 120.9480 - val_loss: 121.9222\n",
      "\n",
      "Epoch 00034: Learning rate is 0.0100.\n",
      "Epoch 35/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 120.3933 - val_loss: 121.3547\n",
      "\n",
      "Epoch 00035: Learning rate is 0.0100.\n",
      "Epoch 36/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 119.6815 - val_loss: 120.7932\n",
      "\n",
      "Epoch 00036: Learning rate is 0.0100.\n",
      "Epoch 37/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 119.0798 - val_loss: 120.2249\n",
      "\n",
      "Epoch 00037: Learning rate is 0.0100.\n",
      "Epoch 38/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 118.4609 - val_loss: 119.7903\n",
      "\n",
      "Epoch 00038: Learning rate is 0.0100.\n",
      "Epoch 39/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 117.7731 - val_loss: 119.2149\n",
      "\n",
      "Epoch 00039: Learning rate is 0.0100.\n",
      "Epoch 40/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 117.0709 - val_loss: 118.0557\n",
      "\n",
      "Epoch 00040: Learning rate is 0.0100.\n",
      "Epoch 41/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 116.3763 - val_loss: 117.5000\n",
      "\n",
      "Epoch 00041: Learning rate is 0.0100.\n",
      "Epoch 42/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 115.7758 - val_loss: 116.9984\n",
      "\n",
      "Epoch 00042: Learning rate is 0.0100.\n",
      "Epoch 43/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 115.2222 - val_loss: 116.4313\n",
      "\n",
      "Epoch 00043: Learning rate is 0.0100.\n",
      "Epoch 44/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 114.5119 - val_loss: 115.8376\n",
      "\n",
      "Epoch 00044: Learning rate is 0.0100.\n",
      "Epoch 45/135\n",
      "625/625 [==============================] - 72s 114ms/step - loss: 113.7695 - val_loss: 114.9435\n",
      "\n",
      "Epoch 00045: Learning rate is 0.0100.\n",
      "Epoch 46/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 112.9634 - val_loss: 113.8597\n",
      "\n",
      "Epoch 00046: Learning rate is 0.0100.\n",
      "Epoch 47/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 111.9707 - val_loss: 113.3181\n",
      "\n",
      "Epoch 00047: Learning rate is 0.0100.\n",
      "Epoch 48/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 111.0312 - val_loss: 112.2409\n",
      "\n",
      "Epoch 00048: Learning rate is 0.0100.\n",
      "Epoch 49/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 109.9419 - val_loss: 111.1980\n",
      "\n",
      "Epoch 00049: Learning rate is 0.0100.\n",
      "Epoch 50/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 108.5862 - val_loss: 109.2621\n",
      "\n",
      "Epoch 00050: Learning rate is 0.0100.\n",
      "Epoch 51/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 107.2620 - val_loss: 108.7397\n",
      "\n",
      "Epoch 00051: Learning rate is 0.0100.\n",
      "Epoch 52/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 105.8586 - val_loss: 106.8005\n",
      "\n",
      "Epoch 00052: Learning rate is 0.0100.\n",
      "Epoch 53/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 104.5709 - val_loss: 106.4534\n",
      "\n",
      "Epoch 00053: Learning rate is 0.0100.\n",
      "Epoch 54/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 104.4901 - val_loss: 107.7528\n",
      "\n",
      "Epoch 00054: Learning rate is 0.0100.\n",
      "Epoch 55/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 108.4996 - val_loss: 111.6716\n",
      "\n",
      "Epoch 00055: Learning rate is 0.0100.\n",
      "Epoch 56/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 108.8455 - val_loss: 110.1955\n",
      "\n",
      "Epoch 00056: Learning rate is 0.0100.\n",
      "Epoch 57/135\n",
      "625/625 [==============================] - 67s 108ms/step - loss: 106.0227 - val_loss: 105.0796\n",
      "\n",
      "Epoch 00057: Learning rate is 0.0100.\n",
      "Epoch 58/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 103.0892 - val_loss: 104.9081\n",
      "\n",
      "Epoch 00058: Learning rate is 0.0100.\n",
      "Epoch 59/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 102.7577 - val_loss: 104.8206\n",
      "\n",
      "Epoch 00059: Learning rate is 0.0100.\n",
      "Epoch 60/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 102.4792 - val_loss: 104.7351\n",
      "\n",
      "Epoch 00060: Learning rate is 0.0100.\n",
      "Epoch 61/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 102.5670 - val_loss: 104.6844\n",
      "\n",
      "Epoch 00061: Learning rate is 0.0100.\n",
      "Epoch 62/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 101.6797 - val_loss: 104.6427\n",
      "\n",
      "Epoch 00062: Learning rate is 0.0100.\n",
      "Epoch 63/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 101.9482 - val_loss: 104.6291\n",
      "\n",
      "Epoch 00063: Learning rate is 0.0100.\n",
      "Epoch 64/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 101.4512 - val_loss: 104.1511\n",
      "\n",
      "Epoch 00064: Learning rate is 0.0100.\n",
      "Epoch 65/135\n",
      "625/625 [==============================] - 67s 107ms/step - loss: 101.7363 - val_loss: 104.5696\n",
      "\n",
      "Epoch 00065: Learning rate is 0.0100.\n",
      "Epoch 66/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 115.7281 - val_loss: 93.1337\n",
      "\n",
      "Epoch 00066: Learning rate is 0.0100.\n",
      "Epoch 67/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 100.1314 - val_loss: 111.2659\n",
      "\n",
      "Epoch 00067: Learning rate is 0.0100.\n",
      "Epoch 68/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 109.1500 - val_loss: 110.8073\n",
      "\n",
      "Epoch 00068: Learning rate is 0.0100.\n",
      "Epoch 69/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 109.0162 - val_loss: 110.7661\n",
      "\n",
      "Epoch 00069: Learning rate is 0.0100.\n",
      "Epoch 70/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 109.0150 - val_loss: 110.7259\n",
      "\n",
      "Epoch 00070: Learning rate is 0.0100.\n",
      "Epoch 71/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 109.0575 - val_loss: 110.6882\n",
      "\n",
      "Epoch 00071: Learning rate is 0.0100.\n",
      "Epoch 72/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 109.0699 - val_loss: 110.9456\n",
      "\n",
      "Epoch 00072: Learning rate is 0.0100.\n",
      "Epoch 73/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 109.0533 - val_loss: 110.9120\n",
      "\n",
      "Epoch 00073: Learning rate is 0.0100.\n",
      "Epoch 74/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 109.1356 - val_loss: 110.8769\n",
      "\n",
      "Epoch 00074: Learning rate is 0.0100.\n",
      "Epoch 75/135\n",
      "625/625 [==============================] - 66s 105ms/step - loss: 109.1191 - val_loss: 110.8286\n",
      "\n",
      "Epoch 00075: Learning rate is 0.0010.\n",
      "Epoch 76/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.1684 - val_loss: 110.8259\n",
      "\n",
      "Epoch 00076: Learning rate is 0.0010.\n",
      "Epoch 77/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.1309 - val_loss: 110.8180\n",
      "\n",
      "Epoch 00077: Learning rate is 0.0010.\n",
      "Epoch 78/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 109.1445 - val_loss: 110.8126\n",
      "\n",
      "Epoch 00078: Learning rate is 0.0010.\n",
      "Epoch 79/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.1727 - val_loss: 110.8051\n",
      "\n",
      "Epoch 00079: Learning rate is 0.0010.\n",
      "Epoch 80/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.1118 - val_loss: 110.7958\n",
      "\n",
      "Epoch 00080: Learning rate is 0.0010.\n",
      "Epoch 81/135\n",
      "625/625 [==============================] - 67s 106ms/step - loss: 109.0943 - val_loss: 110.7846\n",
      "\n",
      "Epoch 00081: Learning rate is 0.0010.\n",
      "Epoch 82/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.0993 - val_loss: 110.7708\n",
      "\n",
      "Epoch 00082: Learning rate is 0.0010.\n",
      "Epoch 83/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.1200 - val_loss: 110.7556\n",
      "\n",
      "Epoch 00083: Learning rate is 0.0010.\n",
      "Epoch 84/135\n",
      "625/625 [==============================] - 67s 107ms/step - loss: 109.2365 - val_loss: 110.7384\n",
      "\n",
      "Epoch 00084: Learning rate is 0.0010.\n",
      "Epoch 85/135\n",
      "625/625 [==============================] - 67s 107ms/step - loss: 109.1771 - val_loss: 110.7230\n",
      "\n",
      "Epoch 00085: Learning rate is 0.0010.\n",
      "Epoch 86/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.1923 - val_loss: 110.7044\n",
      "\n",
      "Epoch 00086: Learning rate is 0.0010.\n",
      "Epoch 87/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.8918 - val_loss: 109.5518\n",
      "\n",
      "Epoch 00087: Learning rate is 0.0010.\n",
      "Epoch 88/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 108.1474 - val_loss: 109.5074\n",
      "\n",
      "Epoch 00088: Learning rate is 0.0010.\n",
      "Epoch 89/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.2458 - val_loss: 109.5020\n",
      "\n",
      "Epoch 00089: Learning rate is 0.0010.\n",
      "Epoch 90/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.2107 - val_loss: 109.4998\n",
      "\n",
      "Epoch 00090: Learning rate is 0.0010.\n",
      "Epoch 91/135\n",
      "625/625 [==============================] - 67s 107ms/step - loss: 108.1191 - val_loss: 109.4992\n",
      "\n",
      "Epoch 00091: Learning rate is 0.0010.\n",
      "Epoch 92/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.2020 - val_loss: 109.4991\n",
      "\n",
      "Epoch 00092: Learning rate is 0.0010.\n",
      "Epoch 93/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.1210 - val_loss: 109.4991\n",
      "\n",
      "Epoch 00093: Learning rate is 0.0010.\n",
      "Epoch 94/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.1161 - val_loss: 109.4989\n",
      "\n",
      "Epoch 00094: Learning rate is 0.0010.\n",
      "Epoch 95/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.3304 - val_loss: 109.5011\n",
      "\n",
      "Epoch 00095: Learning rate is 0.0010.\n",
      "Epoch 96/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 109.4263 - val_loss: 109.4949\n",
      "\n",
      "Epoch 00096: Learning rate is 0.0010.\n",
      "Epoch 97/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.1958 - val_loss: 109.4948\n",
      "\n",
      "Epoch 00097: Learning rate is 0.0010.\n",
      "Epoch 98/135\n",
      "625/625 [==============================] - 65s 105ms/step - loss: 108.2282 - val_loss: 109.4947\n",
      "\n",
      "Epoch 00098: Learning rate is 0.0010.\n",
      "Epoch 99/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.1958 - val_loss: 109.4946\n",
      "\n",
      "Epoch 00099: Learning rate is 0.0010.\n",
      "Epoch 100/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.2035 - val_loss: 109.4944\n",
      "\n",
      "Epoch 00100: Learning rate is 0.0010.\n",
      "Epoch 101/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.2549 - val_loss: 109.4942\n",
      "\n",
      "Epoch 00101: Learning rate is 0.0010.\n",
      "Epoch 102/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.1326 - val_loss: 109.4940\n",
      "\n",
      "Epoch 00102: Learning rate is 0.0010.\n",
      "Epoch 103/135\n",
      "625/625 [==============================] - 67s 107ms/step - loss: 108.1810 - val_loss: 109.4937\n",
      "\n",
      "Epoch 00103: Learning rate is 0.0010.\n",
      "Epoch 104/135\n",
      "625/625 [==============================] - 65s 104ms/step - loss: 108.2391 - val_loss: 109.4927\n",
      "\n",
      "Epoch 00104: Learning rate is 0.0010.\n",
      "Epoch 105/135\n",
      "625/625 [==============================] - 68s 108ms/step - loss: 108.2082 - val_loss: 109.4936\n",
      "\n",
      "Epoch 00105: Learning rate is 0.0001.\n",
      "Epoch 106/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.1560 - val_loss: 109.4936\n",
      "\n",
      "Epoch 00106: Learning rate is 0.0001.\n",
      "Epoch 107/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.1766 - val_loss: 109.4935\n",
      "\n",
      "Epoch 00107: Learning rate is 0.0001.\n",
      "Epoch 108/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.2185 - val_loss: 109.4934\n",
      "\n",
      "Epoch 00108: Learning rate is 0.0001.\n",
      "Epoch 109/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 108.2107 - val_loss: 109.4933\n",
      "\n",
      "Epoch 00109: Learning rate is 0.0001.\n",
      "Epoch 110/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.2496 - val_loss: 109.4932\n",
      "\n",
      "Epoch 00110: Learning rate is 0.0001.\n",
      "Epoch 111/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 108.1941 - val_loss: 109.4930\n",
      "\n",
      "Epoch 00111: Learning rate is 0.0001.\n",
      "Epoch 112/135\n",
      "625/625 [==============================] - 71s 113ms/step - loss: 108.2043 - val_loss: 109.4928\n",
      "\n",
      "Epoch 00112: Learning rate is 0.0001.\n",
      "Epoch 113/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.2933 - val_loss: 109.4988\n",
      "\n",
      "Epoch 00113: Learning rate is 0.0001.\n",
      "Epoch 114/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.2259 - val_loss: 109.4985\n",
      "\n",
      "Epoch 00114: Learning rate is 0.0001.\n",
      "Epoch 115/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.2163 - val_loss: 109.4981\n",
      "\n",
      "Epoch 00115: Learning rate is 0.0001.\n",
      "Epoch 116/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 108.2579 - val_loss: 109.4976\n",
      "\n",
      "Epoch 00116: Learning rate is 0.0001.\n",
      "Epoch 117/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 108.2196 - val_loss: 109.4981\n",
      "\n",
      "Epoch 00117: Learning rate is 0.0001.\n",
      "Epoch 118/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.2880 - val_loss: 109.4976\n",
      "\n",
      "Epoch 00118: Learning rate is 0.0001.\n",
      "Epoch 119/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.1862 - val_loss: 109.4971\n",
      "\n",
      "Epoch 00119: Learning rate is 0.0001.\n",
      "Epoch 120/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 108.2350 - val_loss: 109.4966\n",
      "\n",
      "Epoch 00120: Learning rate is 0.0001.\n",
      "Epoch 121/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.1964 - val_loss: 109.4951\n",
      "\n",
      "Epoch 00121: Learning rate is 0.0001.\n",
      "Epoch 122/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.2299 - val_loss: 109.4944\n",
      "\n",
      "Epoch 00122: Learning rate is 0.0001.\n",
      "Epoch 123/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.1367 - val_loss: 109.4381\n",
      "\n",
      "Epoch 00123: Learning rate is 0.0001.\n",
      "Epoch 124/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 108.1092 - val_loss: 109.4764\n",
      "\n",
      "Epoch 00124: Learning rate is 0.0001.\n",
      "Epoch 125/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 108.0482 - val_loss: 109.4749\n",
      "\n",
      "Epoch 00125: Learning rate is 0.0001.\n",
      "Epoch 126/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 107.6825 - val_loss: 109.4756\n",
      "\n",
      "Epoch 00126: Learning rate is 0.0001.\n",
      "Epoch 127/135\n",
      "625/625 [==============================] - 73s 116ms/step - loss: 107.5390 - val_loss: 109.0167\n",
      "\n",
      "Epoch 00127: Learning rate is 0.0001.\n",
      "Epoch 128/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 107.2704 - val_loss: 109.0702\n",
      "\n",
      "Epoch 00128: Learning rate is 0.0001.\n",
      "Epoch 129/135\n",
      "625/625 [==============================] - 71s 114ms/step - loss: 104.3966 - val_loss: 104.6863\n",
      "\n",
      "Epoch 00129: Learning rate is 0.0001.\n",
      "Epoch 130/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 104.0650 - val_loss: 104.6796\n",
      "\n",
      "Epoch 00130: Learning rate is 0.0001.\n",
      "Epoch 131/135\n",
      "625/625 [==============================] - 73s 117ms/step - loss: 104.1954 - val_loss: 104.6448\n",
      "\n",
      "Epoch 00131: Learning rate is 0.0001.\n",
      "Epoch 132/135\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 104.2079 - val_loss: 104.6497\n",
      "\n",
      "Epoch 00132: Learning rate is 0.0001.\n",
      "Epoch 133/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 104.0664 - val_loss: 104.6490\n",
      "\n",
      "Epoch 00133: Learning rate is 0.0001.\n",
      "Epoch 134/135\n",
      "625/625 [==============================] - 72s 115ms/step - loss: 104.0568 - val_loss: 104.6486\n",
      "\n",
      "Epoch 00134: Learning rate is 0.0001.\n",
      "Epoch 135/135\n",
      "625/625 [==============================] - 70s 113ms/step - loss: 104.0264 - val_loss: 104.6484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51285bf9a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=my_training_batch_generator,\n",
    "          steps_per_epoch = int(len(X_train) // batch_size),\n",
    "          epochs = 135,\n",
    "          verbose = 1,\n",
    "          workers= 4,\n",
    "          validation_data = my_validation_batch_generator,\n",
    "          validation_steps = int(len(X_val) // batch_size),\n",
    "           callbacks=[\n",
    "              CustomLearningRateScheduler(lr_schedule),\n",
    "              mcp_save\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 448, 448, 3)\n",
      "(1, 7, 7, 30)\n"
     ]
    }
   ],
   "source": [
    "test_datasets = []\n",
    "\n",
    "\n",
    "with open(os.path.join(\"VOCdevkit\", '2007_test.txt'), 'r') as f:\n",
    "    test_datasets = test_datasets + f.readlines()\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for item in test_datasets:\n",
    "  item = item.replace(\"\\n\", \"\").split(\" \")\n",
    "  X_test.append(item[0])\n",
    "  arr = []\n",
    "  for i in range(1, len(item)):\n",
    "    arr.append(item[i])\n",
    "  Y_test.append(arr)\n",
    "  break\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "my_test_batch_generator = My_Custom_Generator(X_test, Y_test, batch_size)\n",
    "\n",
    "\n",
    "x_test, y_test = my_test_batch_generator.__getitem__(0)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 30.4796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.47955894470215"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 448, 448, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "\n",
    "#model.call(x_test,training=False)\n",
    "modeloutput = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCdevkit/VOC2007/JPEGImages/000050.jpg\n",
      "['360,192,381,265,4', '399,181,422,235,4', '270,180,291,247,4', '294,176,312,241,4', '68,96,293,375,14', '1,71,87,332,14', '185,68,259,197,14', '286,64,406,238,14']\n"
     ]
    }
   ],
   "source": [
    "print(X_val[12])\n",
    "print(Y_val[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we should have gotten\n",
      "[0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0.927      0.98533333 0.042      0.17866667 1.        ]\n",
      "[0. 0. 0. 0. 0.]\n",
      "What we got\n",
      "tf.Tensor([0.05337387 0.05337387 0.05337387 0.05337387 0.05337387], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.05337387 0.05337387 0.05337387 0.05337387 0.05337387], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.05337387 0.05337387 0.05337387 0.05337387 0.05337387], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.05337387 0.05337387 0.01963515 0.05337387 0.01963515], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.73105854 0.5        0.5        0.73105854 0.5       ], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.5        0.73105854 0.73105854 0.73105854 0.73105854], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#print(y_test.shape)\n",
    "row = 3\n",
    "col = 3\n",
    "\n",
    "print(\"What we should have gotten\")\n",
    "for i in range(0,26,5):\n",
    "    print(y_test[0,row,col,i:i+5])\n",
    "\n",
    "#print(modeloutput.shape)\n",
    "print(\"What we got\")\n",
    "for i in range(0,26,5):\n",
    "    print(modeloutput[0,row,col,i:i+5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5        0.73105854 0.5        0.73105854 0.73105854 0.5\n",
      "  0.73105854]\n",
      " [0.5        0.5        0.5        0.73105854 0.5        0.73105854\n",
      "  0.5       ]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.73105854\n",
      "  0.5       ]\n",
      " [0.5        0.73105854 0.73105854 0.73105854 0.73105854 0.73105854\n",
      "  0.73105854]\n",
      " [0.5        0.5        0.5        0.5        0.73105854 0.5\n",
      "  0.5       ]\n",
      " [0.73105854 0.5        0.73105854 0.73105854 0.73105854 0.5\n",
      "  0.5       ]\n",
      " [0.73105854 0.5        0.5        0.73105854 0.5        0.5\n",
      "  0.5       ]], shape=(7, 7), dtype=float32)\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(modeloutput[0,:,:,29])\n",
    "\n",
    "print(y_test[0,:,:,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCdevkit/VOC2007/JPEGImages/000012.jpg\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "tf.Tensor(\n",
      "[[[0.02963292 0.02963292 0.02963292 ... 0.73105854 0.5        0.5       ]\n",
      "  [0.06420481 0.06420481 0.06420481 ... 0.5        0.73105854 0.73105854]\n",
      "  [0.02570622 0.06987673 0.06987673 ... 0.73105854 0.73105854 0.5       ]\n",
      "  ...\n",
      "  [0.02570622 0.02570622 0.06987673 ... 0.73105854 0.73105854 0.73105854]\n",
      "  [0.06420481 0.02361963 0.06420481 ... 0.5        0.5        0.5       ]\n",
      "  [0.02819718 0.02819718 0.02819718 ... 0.5        0.73105854 0.73105854]]\n",
      "\n",
      " [[0.02461879 0.02461879 0.06692081 ... 0.5        0.73105854 0.5       ]\n",
      "  [0.06420482 0.06420482 0.02361964 ... 0.73105854 0.73105854 0.5       ]\n",
      "  [0.02689415 0.02689415 0.07310586 ... 0.73105854 0.5        0.5       ]\n",
      "  ...\n",
      "  [0.07310586 0.07310586 0.07310586 ... 0.5        0.5        0.5       ]\n",
      "  [0.03975377 0.03975377 0.03975377 ... 0.73105854 0.5        0.73105854]\n",
      "  [0.02819718 0.07664789 0.07664789 ... 0.73105854 0.73105854 0.5       ]]\n",
      "\n",
      " [[0.02689414 0.02689414 0.02689414 ... 0.5        0.5        0.5       ]\n",
      "  [0.07664789 0.07664789 0.07664789 ... 0.73105854 0.5        0.5       ]\n",
      "  [0.03299275 0.03299275 0.03299275 ... 0.73105854 0.73105854 0.5       ]\n",
      "  ...\n",
      "  [0.06987673 0.06987673 0.02570622 ... 0.5        0.73105854 0.5       ]\n",
      "  [0.02461879 0.0669208  0.0669208  ... 0.73105854 0.73105854 0.73105854]\n",
      "  [0.02361963 0.06420481 0.06420481 ... 0.5        0.5        0.5       ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.03497554 0.03497554 0.03497554 ... 0.73105854 0.5        0.5       ]\n",
      "  [0.02819719 0.07664789 0.02819719 ... 0.5        0.73105854 0.5       ]\n",
      "  [0.02689414 0.07310586 0.07310586 ... 0.73105854 0.5        0.5       ]\n",
      "  ...\n",
      "  [0.02689414 0.02689414 0.02689414 ... 0.5        0.5        0.73105854]\n",
      "  [0.02819718 0.02819718 0.02819718 ... 0.5        0.73105854 0.5       ]\n",
      "  [0.07310586 0.07310586 0.07310586 ... 0.5        0.73105854 0.5       ]]\n",
      "\n",
      " [[0.02570622 0.06987673 0.06987673 ... 0.73105854 0.73105854 0.73105854]\n",
      "  [0.02819718 0.07664789 0.02819718 ... 0.73105854 0.5        0.5       ]\n",
      "  [0.06987673 0.02570622 0.06987673 ... 0.73105854 0.73105854 0.73105854]\n",
      "  ...\n",
      "  [0.05723601 0.05723601 0.02105596 ... 0.73105854 0.5        0.73105854]\n",
      "  [0.02689414 0.07310586 0.07310586 ... 0.73105854 0.5        0.5       ]\n",
      "  [0.06170068 0.02269841 0.06170068 ... 0.73105854 0.5        0.5       ]]\n",
      "\n",
      " [[0.02689414 0.07310586 0.02689414 ... 0.73105854 0.5        0.73105854]\n",
      "  [0.06987673 0.06987673 0.06987673 ... 0.73105854 0.73105854 0.5       ]\n",
      "  [0.02689414 0.02689414 0.07310586 ... 0.73105854 0.5        0.5       ]\n",
      "  ...\n",
      "  [0.05723601 0.05723601 0.05723601 ... 0.73105854 0.73105854 0.5       ]\n",
      "  [0.08968358 0.03299275 0.03299275 ... 0.5        0.73105854 0.5       ]\n",
      "  [0.02461879 0.02461879 0.0669208  ... 0.5        0.5        0.5       ]]], shape=(7, 7, 30), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "\n",
    "sanityinput = x_train\n",
    "sanityoutput = model(sanityinput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we should have gotten\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n",
      "What we got\n",
      "tf.Tensor([0.02819718 0.02819718 0.02819718 0.02819718 0.07664789], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.02819718 0.02819718 0.02819718 0.02819718 0.07664789], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.07664789 0.02819718 0.02819718 0.07664789 0.07664789], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.07664789 0.07664789 0.07664789 0.02819718 0.07664789], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.73105854 0.73105854 0.73105854 0.5        0.73105854], shape=(5,), dtype=float32)\n",
      "tf.Tensor([0.73105854 0.73105854 0.5        0.73105854 0.5       ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "row = 4\n",
    "col = 5\n",
    "\n",
    "print(\"What we should have gotten\")\n",
    "for i in range(0,26,5):\n",
    "    print(y_train[0,row,col,i:i+5])\n",
    "\n",
    "#print(modeloutput.shape)\n",
    "print(\"What we got\")\n",
    "for i in range(0,26,5):\n",
    "    print(sanityoutput[0,row,col,i:i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
